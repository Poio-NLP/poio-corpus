{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and parse data from Wikipedia\n",
    "\n",
    "This notebook downloads and parses the data from Wikipedia. The output will be stored as JSONL files, one article per line.\n",
    "\n",
    "You can download individual Wikipedias or all Wikipedias as specified in the Poio Corpus configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/pbouda/Projects/git-github/poio-corpus')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "ROOT_DIR = pathlib.Path().absolute().parent\n",
    "ROOT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load language list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(os.path.join(ROOT_DIR, \"config.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    language_map = json.load(f)[\"LanguagesISOMap\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = language_map.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or set language list manually for testing purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = [\"bar\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process languages\n",
    "\n",
    "First, we define some helper functions.\n",
    "\n",
    "### Get dump link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def dump_link_from_lang_page(wiki_name, page):\n",
    "    html_page = requests.get(page)\n",
    "    soup = BeautifulSoup(html_page.content, features=\"html.parser\")\n",
    "    all_links = soup('a')\n",
    "    for l in all_links:\n",
    "        match = re.match(\n",
    "            wiki_name + \"-(\\d{8})-pages-articles.xml.bz2\", l.string)\n",
    "        if match:\n",
    "            wiki_date = match.group(1)\n",
    "            dump_link = urllib.parse.urljoin(page, l['href'])\n",
    "            return wiki_date, dump_link\n",
    "    return None, None\n",
    "\n",
    "def get_dump_link(iso_639_1):\n",
    "    url = \"https://dumps.wikimedia.org/backup-index.html\"\n",
    "    wiki_prefix = iso_639_1 + \"wiki\"\n",
    "    html_page = requests.get(url)\n",
    "    soup = BeautifulSoup(html_page.content)\n",
    "\n",
    "    page = None\n",
    "    for link in soup('a'):\n",
    "        if link.string == wiki_prefix:\n",
    "            page = urllib.parse.urljoin(url, link['href'])\n",
    "\n",
    "    # get the link for the dump file\n",
    "    return dump_link_from_lang_page(wiki_prefix, page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def download_dump(dump_link, wiki_name, new_wiki_name):\n",
    "    file_name = dump_link.split('/')[-1]\n",
    "    download_path = os.path.join(ROOT_DIR, \"build\", \"corpus\", new_wiki_name)\n",
    "    if not os.path.exists(download_path):\n",
    "        os.makedirs(download_path)\n",
    "    file_path = os.path.join(download_path, file_name)\n",
    "    if not os.path.exists(file_path):\n",
    "        r = requests.get(dump_link)\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract data with WikiExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def wikipedia_extractor(file_path, new_wiki_name):\n",
    "    out = None; err = None\n",
    "    \n",
    "    proc = subprocess.Popen(\n",
    "        [\n",
    "            sys.executable,\n",
    "            \"WikiExtractor.py\",\n",
    "            file_path,\n",
    "            \"--json\",\n",
    "            \"-q\",\n",
    "            \"-b\", \"100M\",\n",
    "            \"-o\", os.path.join(ROOT_DIR, \"build\", \"corpus\", new_wiki_name, \"extracted\")\n",
    "        ],\n",
    "        stdout=subprocess.PIPE\n",
    "    )\n",
    "    (out, err) = proc.communicate()\n",
    "    return (out, err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing bar...\n",
      "  downloading...\n",
      "  extracting...\n"
     ]
    }
   ],
   "source": [
    "for iso_639_3 in languages:\n",
    "    print(\"Processing {}...\".format(iso_639_3))\n",
    "    iso_639_1 = language_map[iso_639_3]\n",
    "    wiki_date, dump_link = get_dump_link(iso_639_1)\n",
    "    in_wiki_prefix = iso_639_1 + \"wiki\"\n",
    "    out_wiki_prefix = iso_639_3 + \"wiki\"\n",
    "    print(\"  downloading...\")\n",
    "    file_path = download_dump(dump_link, in_wiki_prefix, out_wiki_prefix)\n",
    "    print(\"  extracting...\")\n",
    "    wikipedia_extractor(file_path, out_wiki_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
